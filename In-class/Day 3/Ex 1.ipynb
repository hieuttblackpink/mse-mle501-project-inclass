{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vm0Am3z2GbB"
      },
      "source": [
        "# **MACHINE LEARNING**\n",
        "## **MLE501**\n",
        "### **DAY 3 - EXAMPLE 1**\n",
        "### **Linear regreesion for Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enIyFNTB2XDn"
      },
      "outputs": [],
      "source": [
        "# Import section\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPfgEjcm7o1y"
      },
      "source": [
        "Initial data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDyAUN-D7tZr"
      },
      "outputs": [],
      "source": [
        "# Create synhetic dataset for boundary classification with 2 features\n",
        "np.random.seed(42)\n",
        "num_samples = 100\n",
        "X = np.random.randn(num_samples, 2) # 100 samples, 2 features\n",
        "y = (X[:, 0] + X[:, 1] > 0).astype(int) # Binary classification (0 or 1)\n",
        "\n",
        "# Add intercept term (bias)\n",
        "X_bias = np.c_[np.ones((X.shape[0], 1)), X]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5lKXlf22dlE"
      },
      "source": [
        "Softmax function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soVystP72osh"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "  exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) # Stability improvement\n",
        "  return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9jEDmQV3UIu"
      },
      "source": [
        "Cross-entrophy loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt7U4eeY3Ym7"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y, y_pred):\n",
        "  m = y.shape[0]\n",
        "  log_likelihood = -np.log(y_pred[range(m), y])\n",
        "  loss = np.sum(log_likelihood) / m\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYh1d44Q3vv4"
      },
      "source": [
        "Gradient computation for weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvA0aTmM3y_a"
      },
      "outputs": [],
      "source": [
        "def compute_gradients(X, y, y_pred):\n",
        "  m = X.shape[0]\n",
        "  y_onehot = np.zeros_like(y_pred)\n",
        "  y_onehot[np.arange(m), y] = 1 # One-hot encoding\n",
        "  gradients = np.dot(X.T, (y_pred - y_onehot)) / m\n",
        "  return gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11mPt09K6Bed"
      },
      "source": [
        "Plot decision boudary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IekYBYAw6Eem"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(X, y, weights):\n",
        "  plt.figure(figsize=(8, 6))\n",
        "\n",
        "  # Plot data points\n",
        "  plt.scatter(X[:, 1], X[:, 2], c=y, cmap='viridis', edgecolors='k')\n",
        "\n",
        "  # Plot decision bondary\n",
        "  x_values = [np.min(X[:, 1] - 1), np.max(X[:, 1] + 1)]\n",
        "  y_values = -(weights[0, 1] + np.dot(weights[1, 1], x_values)) / weights[2, 1]\n",
        "\n",
        "  plt.plot(x_values, y_values, 'r--', label='Decision Boundary')\n",
        "  plt.xlabel('Feature 1')\n",
        "  plt.ylabel('Feature 2')\n",
        "  plt.legend()\n",
        "  plt.title('Softmax Classifier Decision Boundary')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuowwQT_7KIM"
      },
      "source": [
        "Plot loss over epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoyqjeOb7Non"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses):\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(losses)\n",
        "  plt.title('Loss over Epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTMjY0ds4XkT"
      },
      "source": [
        "Training the linear model with softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4VfvPoe4c7A"
      },
      "outputs": [],
      "source": [
        "def train(X, y, learning_rate=0.0001, epochs=1000):\n",
        "  # m, n = X.shape\n",
        "  # num_classes = len(np.unique(y))\n",
        "\n",
        "  num_classes = 2 # Binary classification\n",
        "  weights = np.zeros((X.shape[1], num_classes)) # Initialize weights\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # Compute logits (raw scores)\n",
        "    logits = np.dot(X, weights)\n",
        "\n",
        "    # Apply softmax to compute probabilities\n",
        "    y_pred = softmax(logits)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(y, y_pred)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = compute_gradients(X, y, y_pred)\n",
        "\n",
        "    # Update weights\n",
        "    weights -= learning_rate * gradients\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "      print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "      plot_decision_boundary(X_bias, y, weights)\n",
        "\n",
        "  return weights, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9aVRhpf8tMK"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IpqBZLVc8vSi",
        "outputId": "5f0f8f45-b96e-4e3a-cab1-a30ac2e872ac"
      },
      "outputs": [],
      "source": [
        "# Train the linear model with softmax classifier\n",
        "weights, losses = train(X_bias, y, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Plot the result\n",
        "plot_decision_boundary(X_bias, y, weights)\n",
        "plot_loss(losses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
